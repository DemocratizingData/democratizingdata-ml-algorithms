{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook looks at the output from the snippet repository\n",
    "and how to use it to train NER, classification, and mlm models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from importlib import reload\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from spacy import displacy\n",
    "import src.data.snippet_repository as sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Enitiy Recognition Models (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_repo = sr.SnippetRepository(sr.SnippetRepositoryMode.NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data = ner_repo.get_training_data(batch_size=10)\n",
    "detected = False\n",
    "while not detected:\n",
    "    ner_df = next(ner_data)\n",
    "    detected = any(ner_df['ner_tags'].apply(lambda ner_tags: any(map(lambda t: t!=\"O\", ner_tags))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Elementary and Secondary Mathematics and Science Education Two States ' Performance on TIMSS : 2007 Massachusetts and Minnesota participated in a special benchmarking study included in the \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Dataset</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Trends in International Mathematics and Science Study \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Dataset</span>\n",
       "</mark>\n",
       "( TIMSS ) 2007 , along with three Canadian provinces , the city of Dubai , and one region of Spain .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = ner_df.iloc[9].text\n",
    "ner_tags = ner_df.iloc[9].ner_tags\n",
    "sr.visualize_ner_tags(text, ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_repo = sr.SnippetRepository(sr.SnippetRepositoryMode.MASKED_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_data = mlm_repo.get_training_data(batch_size=10, balance_labels=True)\n",
    "detected = False\n",
    "while not detected:\n",
    "    mlm_df = next(mlm_data)\n",
    "    detected = any(mlm_df['mask'].apply(lambda token_masks: any(token_masks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>mask</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[However, ,, the, percentages, of, blacks, ear...</td>\n",
       "      <td>[RB, ,, DT, NNS, IN, NNS, VBG, NNP, NN, POS, N...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5b, shows, DeKalb, County, ,, AL, ,, which, i...</td>\n",
       "      <td>[NNP, VBZ, NNP, NNP, ,, NNP, ,, WDT, IN, DT, J...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[TIMSS, =, Trends, in, International, Mathemat...</td>\n",
       "      <td>[NNP, IN, NNS, IN, NNP, NNP, CC, NNP, NNP, .]</td>\n",
       "      <td>[False, False, True, True, True, True, True, T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[There, is, only, one, accuracy, for, the, fif...</td>\n",
       "      <td>[EX, VBZ, RB, CD, NN, IN, DT, JJ, NN, IN, DT, ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[x, G, :, Figure, 1, summarizes, the, relation...</td>\n",
       "      <td>[NFP, NN, :, NN, CD, VBZ, DT, NNS, IN, NNS, .]</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Percentage, distribution, of, ever, married, ...</td>\n",
       "      <td>[NN, NN, IN, RB, VBN, CD, SYM, CD, NN, JJ, NNS...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[A, variety, of, different, mixed, -, effects,...</td>\n",
       "      <td>[DT, NN, IN, JJ, JJ, HYPH, NNS, NNS, ,, DT, NN...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[SOURCES, :, National, Science, Foundation, ,,...</td>\n",
       "      <td>[NNS, :, NNP, NNP, NNP, ,, NNP, NNP, IN, NNP, ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Data, come, from, a, positively, selected, sa...</td>\n",
       "      <td>[NNS, VBP, IN, DT, RB, VBN, NN, IN, JJ, NNS, W...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Table, S6, Data, used, in, preparation, of, t...</td>\n",
       "      <td>[NN, NNP, NNP, VBN, IN, NN, IN, DT, NN, VBD, V...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [However, ,, the, percentages, of, blacks, ear...   \n",
       "1  [5b, shows, DeKalb, County, ,, AL, ,, which, i...   \n",
       "2  [TIMSS, =, Trends, in, International, Mathemat...   \n",
       "3  [There, is, only, one, accuracy, for, the, fif...   \n",
       "4  [x, G, :, Figure, 1, summarizes, the, relation...   \n",
       "5  [Percentage, distribution, of, ever, married, ...   \n",
       "6  [A, variety, of, different, mixed, -, effects,...   \n",
       "7  [SOURCES, :, National, Science, Foundation, ,,...   \n",
       "8  [Data, come, from, a, positively, selected, sa...   \n",
       "9  [Table, S6, Data, used, in, preparation, of, t...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [RB, ,, DT, NNS, IN, NNS, VBG, NNP, NN, POS, N...   \n",
       "1  [NNP, VBZ, NNP, NNP, ,, NNP, ,, WDT, IN, DT, J...   \n",
       "2      [NNP, IN, NNS, IN, NNP, NNP, CC, NNP, NNP, .]   \n",
       "3  [EX, VBZ, RB, CD, NN, IN, DT, JJ, NN, IN, DT, ...   \n",
       "4     [NFP, NN, :, NN, CD, VBZ, DT, NNS, IN, NNS, .]   \n",
       "5  [NN, NN, IN, RB, VBN, CD, SYM, CD, NN, JJ, NNS...   \n",
       "6  [DT, NN, IN, JJ, JJ, HYPH, NNS, NNS, ,, DT, NN...   \n",
       "7  [NNS, :, NNP, NNP, NNP, ,, NNP, NNP, IN, NNP, ...   \n",
       "8  [NNS, VBP, IN, DT, RB, VBN, NN, IN, JJ, NNS, W...   \n",
       "9  [NN, NNP, NNP, VBN, IN, NN, IN, DT, NN, VBD, V...   \n",
       "\n",
       "                                                mask  label  \n",
       "0  [False, False, False, False, False, False, Fal...      0  \n",
       "1  [False, False, False, False, False, False, Fal...      0  \n",
       "2  [False, False, True, True, True, True, True, T...      1  \n",
       "3  [False, False, False, False, False, False, Fal...      0  \n",
       "4  [False, False, False, False, False, False, Fal...      0  \n",
       "5  [False, False, False, False, False, False, Fal...      1  \n",
       "6  [False, False, False, False, False, False, Fal...      0  \n",
       "7  [False, False, False, False, False, False, Fal...      1  \n",
       "8  [False, False, False, False, False, False, Fal...      0  \n",
       "9  [False, False, False, False, False, False, Fal...      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b02d74a0ca42ac8ad6b62ec4a5447c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94e3566c8664a00b9ff8fb53b7d1ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0372f7f621cc4283ad23750348c28b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35e58eb814d49f4b1281088d7d3b058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d03c42fc60541afab8576f7d8355078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import starmap\n",
    "from typing import Any, Callable, Dict, Tuple\n",
    "\n",
    "import datasets\n",
    "import transformers as tfs\n",
    "\n",
    "def apply_mask_sample(tokens:List[str], mask_token_indicator:List[float]) -> List[str]:\n",
    "    tokens = list(map(\n",
    "        lambda t, m: \"[MASK]\" if m else t, \n",
    "        tokens, \n",
    "        mask_token_indicator\n",
    "    ))\n",
    "    return tokens\n",
    "\n",
    "def group_mask_sample(tokens:List[str], mask_token_indicator:List[float]) -> List[str]:\n",
    "   \n",
    "    # group the masks\n",
    "    grouped_text_masks = [tokens[0]]\n",
    "    grouped_mask_token_indicator = [mask_token_indicator[0]]\n",
    "    \n",
    "    for index in range(1, len(tokens)):\n",
    "        if not (mask_token_indicator[index] == 1 and mask_token_indicator[index-1] == 1):\n",
    "            grouped_text_masks.append(tokens[index])\n",
    "            grouped_mask_token_indicator.append(mask_token_indicator[index])\n",
    "\n",
    "    return grouped_text_masks, grouped_mask_token_indicator\n",
    "\n",
    "def apply_mask_batched(dataset:Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # inintially every token is masked, however, we want to group them\n",
    "    # so that a single token represents an entire dataset\n",
    "    ungrouped_masks = list(starmap(\n",
    "        apply_mask_sample,\n",
    "        zip(dataset[\"text\"], dataset[\"mask\"]),\n",
    "    ))\n",
    "\n",
    "    text_mask = list(zip(*list(starmap(\n",
    "        group_mask_sample,\n",
    "        zip(ungrouped_masks, dataset[\"mask\"]),\n",
    "    ))))\n",
    "\n",
    "    dataset[\"text\"], dataset[\"mask\"] = list(text_mask[0]), list(text_mask[1])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(tokenizer_f:Callable, examples:Dict[str, Any]) -> Dict[str, Any]:\n",
    "    tokenized_inputs = tokenizer_f(examples[\"text\"])\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"mask_token_indicator\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = [-100] * len(word_ids) # assume all tokens are special\n",
    "        top_word_id = max(map(lambda x: x if x else -1, word_ids))\n",
    "        for word_idx in range(top_word_id + 1):\n",
    "            label_ids[word_ids.index(word_idx)] = label[word_idx]\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"mask_token_indicator\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "def convert_to_T(T:type, vals:List[str]) -> List[float]:\n",
    "    return [T(x) for x in vals]\n",
    "\n",
    "def convert_dataset(\n",
    "    tokenizer_f:Callable, \n",
    "    collator:Callable,\n",
    "    dataset:datasets.Dataset\n",
    ") -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:\n",
    "\n",
    "    convert_f = partial(convert_to_T, int)\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        lambda dset: { \"mask_token_indicator\" : list(map(convert_f, dset[\"mask\"]))},\n",
    "        batched=True,\n",
    "    ).map(\n",
    "        partial(tokenize_and_align_labels, tokenizer_f),\n",
    "        batched=True,\n",
    "    ).remove_columns(\n",
    "        [\"text\", \"mask\"]\n",
    "    # we rename mask_token_indicator to labels, because that is what the\n",
    "    # so that the data collator will pad it.\n",
    "    ).rename_column(\n",
    "        \"mask_token_indicator\", \"labels\"\n",
    "    ).rename_column(\n",
    "        \"label\", \"seq_labels\"\n",
    "    )\n",
    "\n",
    "    # the collator doesn't know what to do with the seq_labels, so we\n",
    "    # remove it, and then add it back in after the collator is done.\n",
    "    # The collator also changes our type from Dataset to dict, so we\n",
    "    # we are now working with a dictionary of tensors.\n",
    "    tmp_seq_labels = dataset[\"seq_labels\"]\n",
    "    dataset = collator(list(dataset.remove_columns([\"seq_labels\"])))\n",
    "\n",
    "    dataset_inputs = dict(\n",
    "        input_ids=dataset[\"input_ids\"],\n",
    "        attention_mask=dataset[\"attention_mask\"],\n",
    "    )\n",
    "\n",
    "    dataset_labels = dict(\n",
    "        mask_token_indicator = dataset[\"labels\"],\n",
    "        seq_labels = tmp_seq_labels,\n",
    "    )\n",
    "    # At this point, the dataset is a dictionary of tensors, where the\n",
    "    # tensors are all the same length.\n",
    "\n",
    "    return dataset_inputs, dataset_labels\n",
    "\n",
    "\n",
    "tokenizer = tfs.AutoTokenizer.from_pretrained(\n",
    "    \"distilbert-base-cased\",\n",
    "    do_lower_case = False,\n",
    ")\n",
    "tokenizer_f = partial(tokenizer, is_split_into_words=True, truncation=True)\n",
    "\n",
    "collator = tfs.data.data_collator.DataCollatorForTokenClassification(\n",
    "    tokenizer,\n",
    "    return_tensors=\"pt\",\n",
    "    label_pad_token_id=0,\n",
    ")\n",
    "\n",
    "data = datasets.Dataset.from_pandas(\n",
    "    mlm_df.drop(columns=[\"pos_tags\"])\n",
    ").train_test_split(train_size=3)\n",
    "\n",
    "# these have the columns:\n",
    "#  text, mask, label\n",
    "ds_query, ds_support = data[\"train\"], data[\"test\"]\n",
    "\n",
    "# the suport set has the labels masked out\n",
    "ds_support = ds_support.map(apply_mask_batched, batched=True)\n",
    "\n",
    "\n",
    "dset, lbl = convert_dataset(tokenizer_f, collator, ds_support)\n",
    "dset_query, lbl_query = convert_dataset(tokenizer_f, collator, ds_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   157, 13371, 12480,   134,   103,   119,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,  1438,   117,  1103,  6556,  1116,  1104, 14892,  6957,   156,\n",
       "            111,   142,  8091,   112,   188,  4842,  1121,   145,  9428,  2591,\n",
       "           1116,  1105,  1104,  6098,  1116,  6957,   156,   111,   142,  8091,\n",
       "            112,   188,  4842,  1121,   145,  3048, 27485,  1138,  5799,  1290,\n",
       "           1630,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101, 11389,   156,  1545,  7154,  1215,  1107,  7288,  1104,  1142,\n",
       "           3342,  1127,  3836,  1121,  1103, 24278,   112,   188, 20012,   151,\n",
       "           8816,  8136, 26772, 13508,   113,   103,   114,  8539,   113,  8050,\n",
       "           2605,   119, 25338,  2605,   119,  1366,  1665,   119,  5048,  1358,\n",
       "            114,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,   126,  1830,  2196,  3177,  2428,  1348,  1830,  1391,   117,\n",
       "          18589,   117,  1134,  1107,  1103, 10081,  5580,  1104,  5359,   117,\n",
       "           1107,  1103,  1762,  1104,  1103,  6122,  9304, 20708,  1200,  1821,\n",
       "          21667, 12349,  1805,   117,  1114,  3177,  2428,  1348,  1830,  2111,\n",
       "           1515,  5035,  1821, 21667, 12349,  1104,  1164,  5615,  1568,  1477,\n",
       "           4023,  1557, 25621,   194,  1197,   117,  6142,  1103,  1903,  6256,\n",
       "           1107, 14886,  3152,  1121,   121,   119,   124, 17713,   149,   117,\n",
       "           6142,  1103,  1928,  1107, 14886,  1144,  2776,  2434,  1121,  1213,\n",
       "            121,   119,  3565,  1106,   121,   119,  3383, 17713,   149,  1130,\n",
       "           1346,  1816,   117,   170,   182,  6533,  2772,  3656,  1108,  1508,\n",
       "           1154,  2629,  1118,  1103,  1456,  2938,  1615,  2970,   117,  1106,\n",
       "           6707,  1103,  1295,  1105,  2060,  1104, 16358,  1403,   118, 24478,\n",
       "           3380,  1107,  1103,  1352,  1106,  1115,  3685,  1120,  1103,  1159,\n",
       "           1104,  4035, 11179,  1880,   119,   102],\n",
       "         [  101,  7154,  1435,  1121,   170, 14257,  2700,  6876,  1104, 25506,\n",
       "          10519,  1150,  2068,  1111,  1103,  1305,  9737, 16856, 19720,  5960,\n",
       "            113,  9322,  1424,   117, 17037,  4661, 19263,   117,   111, 10605,\n",
       "          10486,  2836,   117,  1384,   114,   117,  1304,  1374,  1104,  2292,\n",
       "           1127,  2620,  1106,  1129,  1690,  1107,  5224,   119,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,   138,  2783,  1104,  1472,  3216,   118,  3154,  3584,   117,\n",
       "            170,  2076,  1104, 11435, 13117,  3136,  1111,  4892,  5252,  2233,\n",
       "            117,  1138,  1151,  3000,  1105,  1152, 11271,  2871,  1107,  1160,\n",
       "           5402,   131,   113,   122,   114,  1103,  3735,  1348, 13457,  1104,\n",
       "           1103,  7449,  7898,  1105,   113,   123,   114,  1103,  7378,  1785,\n",
       "           1104,  1103,  2235,   119,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,   156,  2346, 19556, 10954,  1708,   131,  1305,  2444,  2974,\n",
       "            117,  1305,  1945,  1111,  2444,  1105,  3939, 10910,   117,   103,\n",
       "            113,  1672,  1201,   114,  1105,  3518,   155,   111,   141,  1105,\n",
       "          13886,  8157,   132,  4447,  1104,  6051, 12504,   117, 15161, 21057,\n",
       "          22249,  2233,   113,  1340,  1381,   114,   119,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mask_token_indicator': tensor([[-100,    0, -100,  ...,    0,    0,    0],\n",
       "         [-100,    0, -100,  ..., -100, -100, -100],\n",
       "         [-100,    0,    0,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [-100,    0,    0,  ...,    0,    0,    0],\n",
       "         [-100,    0,    0,  ...,    0,    0,    0],\n",
       "         [-100,    0, -100,  ...,    0,    0,    0]]),\n",
       " 'seq_labels': [1, 1, 0, 0, 0, 0, 1]}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_special = ((lbl[\"mask_token_indicator\"][0,:] == -100).float() - 1).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset[\"attention_mask\"][0,:] * mask_special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_support[2][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f79e38f4ab240799931e9654ff67bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1ad178b9c947c09044a2278aa34adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels ['seq_label', 'mask_token_indicator', 'attention_mask']\n",
      " features ['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "import transformers as tfs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = datasets.Dataset.from_pandas(\n",
    "    mlm_df.drop(columns=[\"pos_tags\"])\n",
    ").train_test_split(train_size=3)\n",
    "\n",
    "ds_query, ds_support = data[\"train\"], data[\"test\"]\n",
    "\n",
    "ds_query = ds_query.map(\n",
    "    lambda row: { \"mask_token_indicator\" : list(map(convert_to_float, row[\"mask\"]))},\n",
    "    batched=True,\n",
    "    remove_columns=[\"mask\"]\n",
    ").map(\n",
    "    partial(tokenize_and_align_labels, tokenizer_f), \n",
    "    batched=True,\n",
    ").rename_column(\"label\", \"seq_label\").remove_columns([\"text\"])\n",
    "dq_query_labels = ds_query.remove_columns([\"input_ids\"])\n",
    "ds_query = ds_query.remove_columns([\"seq_label\", \"mask_token_indicator\"])\n",
    "\n",
    "print(\n",
    "    \"labels {}\\n\".format(dq_query_labels.column_names),\n",
    "    \"features {}\".format(ds_query.column_names),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (4031507676.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[88], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def apply_mask_sample(tokens:List[str], mask_token_indicator:List[float]) -> List[str]:\n",
    "    return list(map(\n",
    "        lambda t, m: \"[MASK]\" if m else t, \n",
    "        tokens, \n",
    "        mask_token_indicator\n",
    "    ))\n",
    "\n",
    "def apply_mask_batched(sample:Dict[str, Any]) -> Dict[str, Any]:\n",
    "    sample[\"text\"] = apply_mask_sample(sample[\"text\"], sample[\"mask_token_indicator\"])\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['seq_label', 'mask_token_indicator', 'attention_mask']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m collator(\u001b[39mlist\u001b[39;49m(dq_query_labels))\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizing-data-ml-algorithms/lib/python3.10/site-packages/transformers/data/data_collator.py:45\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf_call(features)\n\u001b[1;32m     44\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch_call(features)\n\u001b[1;32m     46\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnp\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizing-data-ml-algorithms/lib/python3.10/site-packages/transformers/data/data_collator.py:308\u001b[0m, in \u001b[0;36mDataCollatorForTokenClassification.torch_call\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    306\u001b[0m label_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m features[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m labels \u001b[39m=\u001b[39m [feature[label_name] \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m features] \u001b[39mif\u001b[39;00m label_name \u001b[39min\u001b[39;00m features[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad(\n\u001b[1;32m    309\u001b[0m     features,\n\u001b[1;32m    310\u001b[0m     padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    311\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_length,\n\u001b[1;32m    312\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    313\u001b[0m     \u001b[39m# Conversion to tensors will fail if we have labels as they are not of the same length yet.\u001b[39;49;00m\n\u001b[1;32m    314\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m labels \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    315\u001b[0m )\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[39mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizing-data-ml-algorithms/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2904\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   2902\u001b[0m \u001b[39m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[1;32m   2903\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m encoded_inputs:\n\u001b[0;32m-> 2904\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2905\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2906\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthat includes \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, but you provided \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(encoded_inputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2907\u001b[0m     )\n\u001b[1;32m   2909\u001b[0m required_input \u001b[39m=\u001b[39m encoded_inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]]\n\u001b[1;32m   2911\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m required_input:\n",
      "\u001b[0;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['seq_label', 'mask_token_indicator', 'attention_mask']"
     ]
    }
   ],
   "source": [
    "collator(list(dq_query_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1438,   117,  1103,  6556,  1116,  1104, 14892,  6957,   156,\n",
       "           111,   142,  8091,   112,   188,  4842,  1121,   145,  9428,  2591,\n",
       "          1116,  1105,  1104,  6098,  1116,  6957,   156,   111,   142,  8091,\n",
       "           112,   188,  4842,  1121,   145,  3048, 27485,  1138,  5799,  1290,\n",
       "          1630,   119,   102,     0],\n",
       "        [  101,   157, 13371, 12480,   134,   157,  5123,  3680,  1107,  1570,\n",
       "          9833,  1105,  2444,  8690,   119,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101, 11389,   156,  1545,  7154,  1215,  1107,  7288,  1104,  1142,\n",
       "          3342,  1127,  3836,  1121,  1103, 24278,   112,   188, 20012,   151,\n",
       "          8816,  8136, 26772, 13508,   113,  5844, 27451,   114,  8539,   113,\n",
       "          8050,  2605,   119, 25338,  2605,   119,  1366,  1665,   119,  5048,\n",
       "          1358,   114,   119,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator(list(ds_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4054649f0a04be8b1bc6c9c575f34d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d99b19225a74dd5bc5b42040e25b8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['seq_label', 'mask_token_indicator', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def apply_mask_sample(tokens:List[str], mask_token_indicator:List[float]) -> List[str]:\n",
    "    return list(map(\n",
    "        lambda t, m: \"[MASK]\" if m else t, \n",
    "        tokens, \n",
    "        mask_token_indicator\n",
    "    ))\n",
    "\n",
    "def apply_mask_batched(sample:Dict[str, Any]) -> Dict[str, Any]:\n",
    "    sample[\"text\"] = apply_mask_sample(sample[\"text\"], sample[\"mask_token_indicator\"])\n",
    "    return sample\n",
    "\n",
    "def tokenize_and_align_labels(tokenizer_f, examples):\n",
    "    tokenized_inputs = tokenizer_f(examples[\"text\"])\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"mask_token_indicator\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = [-100] * len(word_ids) # assume all tokens are special\n",
    "        top_word_id = max(map(lambda x: x if x else -1, word_ids))\n",
    "        for word_idx in range(top_word_id + 1):\n",
    "            label_ids[word_ids.index(word_idx)] = label[word_idx]\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"mask_token_indicator\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def convert_to_float(vals:List[str]) -> List[float]:\n",
    "    return [float(x) for x in vals]\n",
    "\n",
    "keep_cols = ['seq_label', 'mask_token_indicator', 'input_ids', 'attention_mask']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"distilbert-base-cased\",\n",
    "    do_lower_case = False,\n",
    ")\n",
    "tokenizer_f = partial(tokenizer, is_split_into_words=True, truncation=True)\n",
    "\n",
    "collator = tfs.data.data_collator.DataCollatorForTokenClassification(\n",
    "    tokenizer,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "q_dset = datasets.Dataset.from_pandas(\n",
    "    query.drop(columns=[\"pos_tags\"])\n",
    ").map(\n",
    "    lambda row: { \"mask_token_indicator\" : list(map(convert_to_float, row[\"mask\"]))},\n",
    "    batched=True,\n",
    "    remove_columns=[\"mask\"]\n",
    ").map(\n",
    "    partial(tokenize_and_align_labels, tokenizer_f), \n",
    "    batched=True,\n",
    ").rename_column(\"label\", \"seq_label\")\n",
    "\n",
    "q_dset.remove_columns(list(filter(lambda c: c not in keep_cols, q_dset.column_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = datasets.Dataset.from_pandas(\n",
    "    support.drop(columns=[\"pos_tags\"]).rename(columns={\"mask_token_indicator\": \"mask\"})\n",
    ").map("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9e7d7a708a4d5eab62ac52f9029dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets as ds\n",
    "import pandas as pd \n",
    "\n",
    "def make_labels_dataset(pandas:pd.DataFrame) -> ds.Dataset:\n",
    "    keep_cols = [\"text\", \"mask\", \"label\"]\n",
    "\n",
    "    convert_to_float = lambda vals: [float(x) for x in vals]\n",
    "\n",
    "    labelset = ds.Dataset.from_pandas(\n",
    "        pandas.rename(columns={\"mask_token_indicator\": \"mask\"})\n",
    "    ).map(\n",
    "        lambda row: { \"mask_token_indicator\" : list(map(convert_to_float, row[\"mask\"]))},\n",
    "        batched=True,\n",
    "        remove_columns=[\"mask\"]\n",
    "    )\n",
    "\n",
    "    labelset = labelset.remove_columns(list(filter(\n",
    "        lambda c: c not in keep_cols,\n",
    "        labelset.column_names\n",
    "    ))) \n",
    "\n",
    "    return labelset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "make_labels_dataset(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "democratizing-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "194abaa8cfe060ba73100cf06e4f6aa955db251a86425a2f723ab6ae30787673"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
