{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook looks at the output from the snippet repository\n",
    "and how to use it to train NER, classification, and mlm models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "from spacy import displacy\n",
    "import src.data.snippet_repository as sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Enitiy Recognition Models (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_repo = sr.SnippetRepository(sr.SnippetRepositoryMode.NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data = ner_repo.get_training_data(batch_size=10)\n",
    "detected = False\n",
    "while not detected:\n",
    "    ner_df = next(ner_data)\n",
    "    detected = any(ner_df['ner_tags'].apply(lambda ner_tags: any(map(lambda t: t!=\"O\", ner_tags))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Elementary and Secondary Mathematics and Science Education Two States ' Performance on TIMSS : 2007 Massachusetts and Minnesota participated in a special benchmarking study included in the \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Dataset</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Trends in International Mathematics and Science Study \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Dataset</span>\n",
       "</mark>\n",
       "( TIMSS ) 2007 , along with three Canadian provinces , the city of Dubai , and one region of Spain .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = ner_df.iloc[9].text\n",
    "ner_tags = ner_df.iloc[9].ner_tags\n",
    "sr.visualize_ner_tags(text, ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model here\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"dslim/bert-base-NER\",\n",
       "  \"_num_labels\": 9,\n",
       "  \"architectures\": [\n",
       "    \"BertForTokenClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"O\",\n",
       "    \"1\": \"B-MISC\",\n",
       "    \"2\": \"I-MISC\",\n",
       "    \"3\": \"B-PER\",\n",
       "    \"4\": \"I-PER\",\n",
       "    \"5\": \"B-ORG\",\n",
       "    \"6\": \"I-ORG\",\n",
       "    \"7\": \"B-LOC\",\n",
       "    \"8\": \"I-LOC\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"B-LOC\": 7,\n",
       "    \"B-MISC\": 1,\n",
       "    \"B-ORG\": 5,\n",
       "    \"B-PER\": 3,\n",
       "    \"I-LOC\": 8,\n",
       "    \"I-MISC\": 2,\n",
       "    \"I-ORG\": 6,\n",
       "    \"I-PER\": 4,\n",
       "    \"O\": 0\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.20.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoConfig.from_pretrained(\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"dslim/bert-base-NER\",\n",
    "    padding=True, truncation=True,\n",
    "\n",
    ")\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"dslim/bert-base-NER\", \n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 3, 4, 5, 6, 7, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = tokenizer(\n",
    "    [\n",
    "        \"This is sample @paulwalk, text national and it is grand\".split(),\n",
    "        \"Hello this is Me, I am a person\".split(),\n",
    "    ], \n",
    "    is_split_into_words=True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    ")\n",
    "print(features.keys())\n",
    "idx = 1\n",
    "tokenizer.convert_ids_to_tokens(features[\"input_ids\"][idx])\n",
    "features.word_ids(batch_index=idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = model(**features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 768]), torch.Size([1, 12, 10, 10]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.hidden_states[0].shape, outs.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.4593,  0.0671, -0.1690,  ...,  0.0050,  0.0154, -0.0556],\n",
       "          [-1.5191, -0.0687,  0.8964,  ...,  0.4139,  0.3173,  0.3316],\n",
       "          [-1.1648,  0.2270,  0.7271,  ...,  0.4671,  0.9646,  0.7556],\n",
       "          ...,\n",
       "          [-0.7087,  0.4728,  0.6344,  ...,  0.4425,  1.3855,  0.7695],\n",
       "          [-0.8678,  0.2932,  0.3862,  ..., -0.9396, -0.1360,  0.5752],\n",
       "          [ 0.0699,  0.0933,  0.3413,  ...,  0.5569, -0.5428,  0.4209]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.2290, -0.0552, -0.0077,  ..., -0.0809, -0.0217,  0.0072],\n",
       "          [-1.5546, -0.0652,  0.9279,  ...,  0.4970,  0.0371,  0.2910],\n",
       "          [-1.1191,  0.4731,  0.6059,  ...,  0.4033,  0.5977,  0.8683],\n",
       "          ...,\n",
       "          [-0.3756,  0.4558,  0.4442,  ...,  0.4746,  1.0184,  0.9340],\n",
       "          [-0.9685,  0.2611,  0.3602,  ..., -1.4144, -0.7480,  0.7747],\n",
       "          [ 0.1437, -0.4574,  0.4823,  ...,  0.6414, -0.6725,  0.4760]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.3517, -0.1521,  0.0339,  ..., -0.0893, -0.1230, -0.0402],\n",
       "          [-1.2670, -0.3738,  1.2099,  ...,  0.6776, -0.0477,  0.4737],\n",
       "          [-0.6333,  0.0545,  0.7228,  ...,  0.1774,  0.7345,  0.8852],\n",
       "          ...,\n",
       "          [-0.1589,  0.3856, -0.1112,  ...,  0.2170,  0.8159,  0.7138],\n",
       "          [-0.6912, -0.5831,  0.0610,  ..., -1.6318, -0.6512,  1.0126],\n",
       "          [ 0.0154, -0.2261,  0.1721,  ...,  0.1754, -0.2539,  0.0365]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.8029,  0.1422, -0.2297,  ..., -0.3776, -0.0978,  0.1968],\n",
       "          [-1.5378, -0.4223,  1.0376,  ...,  0.6688, -0.3639,  0.7775],\n",
       "          [-0.8042, -0.2304,  0.4107,  ..., -0.0356,  1.1151,  0.8153],\n",
       "          ...,\n",
       "          [-0.2309,  0.3339, -0.2967,  ...,  0.1950,  0.8120,  0.8796],\n",
       "          [-0.2382, -0.6250, -0.0141,  ..., -1.3255, -0.5605,  1.3668],\n",
       "          [ 0.0016, -0.1219,  0.0678,  ..., -0.0286, -0.0854, -0.0075]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.9276,  0.5021, -0.3012,  ..., -0.3072,  0.0585, -0.0952],\n",
       "          [-1.2704, -0.3390,  1.2081,  ...,  0.9522, -0.7325,  0.5915],\n",
       "          [-0.3820, -0.3793,  0.7925,  ...,  0.1308,  0.9165,  0.3711],\n",
       "          ...,\n",
       "          [ 0.0740,  0.0986, -0.3675,  ..., -0.0765,  0.8113,  0.7667],\n",
       "          [ 0.1863, -1.1446,  0.1525,  ..., -1.4336, -0.2183,  0.9785],\n",
       "          [ 0.0383, -0.0120,  0.0225,  ..., -0.0454, -0.0144, -0.0857]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 7.4886e-01,  1.5984e-01, -4.2636e-01,  ...,  3.4759e-01,\n",
       "           -2.3844e-01,  6.5698e-02],\n",
       "          [-1.1580e+00, -8.4134e-01,  1.5521e+00,  ...,  9.8241e-01,\n",
       "           -7.9806e-01,  4.0210e-01],\n",
       "          [-1.3816e-01, -1.6888e-01,  1.1548e+00,  ...,  4.7964e-01,\n",
       "            6.1592e-01,  4.7457e-01],\n",
       "          ...,\n",
       "          [-7.2834e-04,  3.9289e-01,  2.9005e-03,  ...,  5.3336e-02,\n",
       "            8.5969e-01,  8.5849e-01],\n",
       "          [ 4.3715e-01, -9.9634e-01,  1.6856e-01,  ..., -9.4637e-01,\n",
       "           -6.5659e-01,  1.0316e+00],\n",
       "          [-2.1963e-02, -5.5799e-02,  6.2227e-02,  ..., -2.8402e-02,\n",
       "           -4.2231e-02, -1.0502e-01]]], grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.7793,  0.1007, -0.4274,  ...,  0.9048, -0.5227, -0.1137],\n",
       "          [-0.8898, -0.6786,  1.5308,  ...,  1.5091, -0.9785,  0.6784],\n",
       "          [ 0.0086,  0.0768,  1.3342,  ...,  0.9391,  0.4556,  0.6326],\n",
       "          ...,\n",
       "          [-0.2714,  0.9064,  0.0705,  ...,  0.6570,  0.5663,  1.0624],\n",
       "          [-0.1689, -0.8354,  0.0207,  ..., -0.8175, -0.1637,  1.1326],\n",
       "          [-0.0286, -0.0303,  0.0348,  ..., -0.0376, -0.0249, -0.1028]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.5420, -0.0111, -0.4392,  ...,  0.8796, -0.4320,  0.1476],\n",
       "          [-0.8182, -0.0612,  0.9416,  ...,  1.1617, -0.5889,  0.5871],\n",
       "          [-0.0602,  0.7197,  1.2016,  ...,  0.3607,  0.4780,  0.5487],\n",
       "          ...,\n",
       "          [-0.1530,  0.9255,  0.2372,  ...,  0.8295,  0.8583,  1.3533],\n",
       "          [ 0.1905, -0.5354, -0.1243,  ..., -0.5397, -0.3394,  1.1881],\n",
       "          [-0.0707,  0.0226,  0.0754,  ...,  0.0392, -0.0574, -0.0782]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.3221,  0.3998, -0.4169,  ...,  1.0232, -0.5995, -0.0601],\n",
       "          [-1.0271,  0.6184,  1.0531,  ...,  1.9698, -0.6685,  0.6509],\n",
       "          [-0.0514,  1.3279,  1.0187,  ...,  1.0334,  0.2102,  0.3808],\n",
       "          ...,\n",
       "          [-0.1540,  1.3046,  0.5710,  ...,  1.4849,  0.7665,  1.7083],\n",
       "          [-0.1558, -0.1113, -0.1119,  ..., -0.5500, -0.1215,  1.2044],\n",
       "          [-0.0054,  0.0390,  0.0153,  ...,  0.0636, -0.0282, -0.0825]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.8809,  0.7892, -0.4482,  ...,  1.1711,  0.6975, -0.0958],\n",
       "          [ 0.3349,  0.8297,  1.7645,  ...,  1.9044,  0.5226,  1.2158],\n",
       "          [ 0.4887,  1.2486,  1.0644,  ...,  1.3924,  0.7629,  1.0750],\n",
       "          ...,\n",
       "          [ 0.0372,  0.9547,  0.3251,  ...,  1.5483,  0.5109,  1.6606],\n",
       "          [ 0.6295,  0.0969, -0.1112,  ..., -0.1451, -0.3940,  1.2274],\n",
       "          [-0.0465,  0.0894,  0.0296,  ...,  0.0093, -0.0322, -0.0873]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.3667,  0.4766, -1.0618,  ...,  0.8177,  0.6255,  0.0863],\n",
       "          [ 0.6374,  0.4711,  1.5596,  ...,  0.4840,  0.6340,  0.6172],\n",
       "          [ 0.7630,  0.5707,  1.1389,  ...,  0.2344,  0.7438,  0.4282],\n",
       "          ...,\n",
       "          [ 0.1127,  0.3572,  0.4659,  ...,  0.4194,  0.5050,  0.7742],\n",
       "          [ 0.5304, -0.3046, -0.1775,  ..., -0.1200, -0.2799,  0.7286],\n",
       "          [-0.0110,  0.1009,  0.0099,  ..., -0.0421, -0.0176, -0.0560]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[-2.5155e-01,  3.8445e-01, -8.2529e-01,  ...,  3.4392e-01,\n",
       "            7.1555e-01,  7.4832e-01],\n",
       "          [-4.4567e-02,  2.7308e-01,  8.6723e-01,  ...,  4.1996e-01,\n",
       "            6.8162e-01,  8.6585e-01],\n",
       "          [-2.3420e-02,  3.7355e-01,  7.1472e-01,  ...,  4.6169e-02,\n",
       "            9.6745e-01,  6.2572e-01],\n",
       "          ...,\n",
       "          [-6.8192e-01,  2.1714e-01,  3.3690e-01,  ...,  3.1651e-01,\n",
       "            7.7462e-01,  8.4944e-01],\n",
       "          [-2.1414e-01, -2.8565e-01, -1.3242e-01,  ...,  2.2176e-01,\n",
       "            1.9459e-04,  7.2631e-01],\n",
       "          [-1.1367e-01,  1.0859e-01,  4.0207e-02,  ..., -8.7839e-03,\n",
       "           -1.0682e-03, -7.4373e-02]]], grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.1054, -0.3361,  0.7459,  ..., -0.0819,  0.4926,  0.2336],\n",
       "          [ 0.2352, -0.4984,  1.0282,  ..., -0.0698,  0.4869,  0.4297],\n",
       "          [ 0.2079, -0.2714,  0.9021,  ..., -0.1112,  0.5088,  0.3131],\n",
       "          ...,\n",
       "          [-0.0488, -0.4109,  0.6926,  ..., -0.0351,  0.4354,  0.3481],\n",
       "          [-0.0329, -0.5736,  0.6474,  ..., -0.4038,  0.2286,  0.5712],\n",
       "          [-0.3632, -0.2771,  0.9473,  ..., -0.9103,  0.2297, -0.1166]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(ner_df.text.tolist(), is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(ner_df.drop(columns=[\"tags\"]).rename(columns={\"ner_tags\": \"labels\"}))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443b512b0ead4c9fbed0f5a7316e9051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_to_id = {\"O\":0, \"B-DAT\":1, \"I-DAT\":2}\n",
    "id_to_lbl = {v:k for k,v in lbl_to_id.items()}\n",
    "\n",
    "def convert_ner_tags_to_ids(ner_tags: List[str]) -> List[int]:\n",
    "    return [lbl_to_id[ner_tag] for ner_tag in ner_tags]\n",
    "\n",
    "ds_with_int_labels = ds.map(lambda x: {\"labels\":  [convert_ner_tags_to_ids(lbls) for lbls in x[\"labels\"]]}, batched=True)\n",
    "ds_with_int_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(tokenizer_f, examples):\n",
    "    tokenized_inputs = tokenizer_f(examples[\"text\"])\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = [-100] * len(word_ids) # assume all tokens are special\n",
    "        top_word_id = max(map(lambda x: x if x else -1, word_ids))\n",
    "        for word_idx in range(top_word_id + 1):\n",
    "            label_ids[word_ids.index(word_idx)] = label[word_idx]\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ed3a1bc7384e6abc0085174e929923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_f = partial(tokenizer, is_split_into_words=True, truncation=True)\n",
    "ds_tokenized = ds_with_int_labels.map(partial(tokenize_and_align_labels, tokenize_f), batched=True, remove_columns=[\"text\"])\n",
    "ds_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bb35f4b0104c6fa87316331c56b165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "lbl_to_id = {\"O\":0, \"B-DAT\":1, \"I-DAT\":2}\n",
    "id_to_lbl = {v:k for k,v in lbl_to_id.items()}\n",
    "tokenize_f = partial(tokenizer, is_split_into_words=True, truncation=True)\n",
    "convert_label_f = lambda ner_tags: list(map(lambda tag: lbl_to_id[tag], ner_tags))\n",
    "\n",
    "formatted_ds = ds.map(\n",
    "    lambda sample: tokenize_f(sample[\"text\"]) | {\"labels\": convert_label_f(sample[\"labels\"])},\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "formatted_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "data = DataLoader(ds_tokenized, batch_size=1, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([[-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, -100]]), 'input_ids': tensor([[  101,  1130,  1901,  1106, 11621,  1704,  6233,  5814,  1869,   117,\n",
      "          2304, 24730,  4454,  1145,  4465,  2233,  1113,  1216,  3050,  1112,\n",
      "          1705,  2060,   117,  3872,  3188,   117,  1105,  1295,  1104,  1278,\n",
      "          6461,  1296,  1768,  1108,  3188,   117,  1111,  1296,  3397,  1214,\n",
      "          1103,  3218,  1108,  4071,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "for d in data:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 46]), torch.Size([1, 46]), torch.Size([1, 46]))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"labels\"].shape, d[\"input_ids\"].shape, d[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_ds[0][\"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatted_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizingdata-ml-algo/lib/python3.10/site-packages/transformers/data/data_collator.py:42\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf_call(features)\n\u001b[1;32m     41\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch_call(features)\n\u001b[1;32m     43\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnp\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizingdata-ml-algo/lib/python3.10/site-packages/transformers/data/data_collator.py:306\u001b[0m, in \u001b[0;36mDataCollatorForTokenClassification.torch_call\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    304\u001b[0m label_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m features[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m labels \u001b[39m=\u001b[39m [feature[label_name] \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m features] \u001b[39mif\u001b[39;00m label_name \u001b[39min\u001b[39;00m features[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad(\n\u001b[1;32m    307\u001b[0m     features,\n\u001b[1;32m    308\u001b[0m     padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    309\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_length,\n\u001b[1;32m    310\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    311\u001b[0m     \u001b[39m# Conversion to tensors will fail if we have labels as they are not of the same length yet.\u001b[39;49;00m\n\u001b[1;32m    312\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m labels \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    313\u001b[0m )\n\u001b[1;32m    315\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[39mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizingdata-ml-algo/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2815\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[39m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[1;32m   2812\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m encoded_inputs:\n\u001b[1;32m   2813\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2814\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 2815\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthat includes \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, but you provided \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(encoded_inputs\u001b[39m.\u001b[39;49mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2816\u001b[0m     )\n\u001b[1;32m   2818\u001b[0m required_input \u001b[39m=\u001b[39m encoded_inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]]\n\u001b[1;32m   2820\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m required_input:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "data_collator(formatted_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[0, 1, 2, 0, 0, 0],\n",
       "         [0, 1, 2, 3, 4, 5]]),\n",
       " 'labels': tensor([[   0,    1,    2, -100, -100, -100],\n",
       "         [   0,    1,    2,    3,    4,    5]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\n",
    "    {\"input_ids\": [0, 1, 2], \"labels\": [0, 1, 2]},\n",
    "    {\"input_ids\": [0, 1, 2, 3, 4, 5], \"labels\": [0, 1, 2, 3, 4, 5]},\n",
    "]\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('democratizingdata-ml-algo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3e03aa19f4cee1cca181f326aea7726af1ea307b5fcbfd54a41f945b089b370"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
