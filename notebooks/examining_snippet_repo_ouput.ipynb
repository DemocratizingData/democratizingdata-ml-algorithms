{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook looks at the output from the snippet repository\n",
    "and how to use it to train NER, classification, and mlm models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import List\n",
    "import torch\n",
    "from spacy import displacy\n",
    "import src.data.snippet_repository as sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Enitiy Recognition Models (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_repo = sr.SnippetRepository(sr.SnippetRepositoryMode.NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data = ner_repo.get_training_data(batch_size=10)\n",
    "detected = False\n",
    "while not detected:\n",
    "    ner_df = next(ner_data)\n",
    "    detected = any(ner_df['ner_tags'].apply(lambda ner_tags: any(map(lambda t: t!=\"O\", ner_tags))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Elementary and Secondary Mathematics and Science Education Two States ' Performance on TIMSS : 2007 Massachusetts and Minnesota participated in a special benchmarking study included in the \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Dataset</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Trends in International Mathematics and Science Study \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Dataset</span>\n",
       "</mark>\n",
       "( TIMSS ) 2007 , along with three Canadian provinces , the city of Dubai , and one region of Spain .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = ner_df.iloc[9].text\n",
    "ner_tags = ner_df.iloc[9].ner_tags\n",
    "sr.visualize_ner_tags(text, ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model here\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"dslim/bert-base-NER\",\n",
       "  \"_num_labels\": 9,\n",
       "  \"architectures\": [\n",
       "    \"BertForTokenClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"O\",\n",
       "    \"1\": \"B-MISC\",\n",
       "    \"2\": \"I-MISC\",\n",
       "    \"3\": \"B-PER\",\n",
       "    \"4\": \"I-PER\",\n",
       "    \"5\": \"B-ORG\",\n",
       "    \"6\": \"I-ORG\",\n",
       "    \"7\": \"B-LOC\",\n",
       "    \"8\": \"I-LOC\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"B-LOC\": 7,\n",
       "    \"B-MISC\": 1,\n",
       "    \"B-ORG\": 5,\n",
       "    \"B-PER\": 3,\n",
       "    \"I-LOC\": 8,\n",
       "    \"I-MISC\": 2,\n",
       "    \"I-ORG\": 6,\n",
       "    \"I-PER\": 4,\n",
       "    \"O\": 0\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoConfig.from_pretrained(\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"dslim/bert-base-NER\",\n",
    "    padding=True, truncation=True,\n",
    "\n",
    ")\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"dslim/bert-base-NER\", \n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 3, 4, 5, 6, 7, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = tokenizer(\n",
    "    [\n",
    "        \"This is sample @paulwalk, text national and it is grand\".split(),\n",
    "        \"Hello this is Me, I am a person\".split(),\n",
    "    ], \n",
    "    is_split_into_words=True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    ")\n",
    "print(features.keys())\n",
    "idx = 1\n",
    "tokenizer.convert_ids_to_tokens(features[\"input_ids\"][idx])\n",
    "features.word_ids(batch_index=idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.input_ids.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.ones([2, 16], dtype=torch.long)\n",
    "outs = model(labels=labels, **features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4103, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 9]) torch.Size([2, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(10.4103, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(outs.logits.size(), labels.size())\n",
    "torch.nn.functional.cross_entropy(\n",
    "    outs.logits.view(-1, outs.logits.size(-1)), \n",
    "    labels.view(-1), \n",
    "    reduce=False,\n",
    "    ignore_index=-100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 9])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(outs.logits.view(-1, outs.logits.size(-1)).size())\n",
    "print(labels.view(-1).size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 768]), torch.Size([1, 12, 10, 10]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.hidden_states[0].shape, outs.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.4593,  0.0671, -0.1690,  ...,  0.0050,  0.0154, -0.0556],\n",
       "          [-1.5191, -0.0687,  0.8964,  ...,  0.4139,  0.3173,  0.3316],\n",
       "          [-1.1648,  0.2270,  0.7271,  ...,  0.4671,  0.9646,  0.7556],\n",
       "          ...,\n",
       "          [-0.7087,  0.4728,  0.6344,  ...,  0.4425,  1.3855,  0.7695],\n",
       "          [-0.8678,  0.2932,  0.3862,  ..., -0.9396, -0.1360,  0.5752],\n",
       "          [ 0.0699,  0.0933,  0.3413,  ...,  0.5569, -0.5428,  0.4209]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.2290, -0.0552, -0.0077,  ..., -0.0809, -0.0217,  0.0072],\n",
       "          [-1.5546, -0.0652,  0.9279,  ...,  0.4970,  0.0371,  0.2910],\n",
       "          [-1.1191,  0.4731,  0.6059,  ...,  0.4033,  0.5977,  0.8683],\n",
       "          ...,\n",
       "          [-0.3756,  0.4558,  0.4442,  ...,  0.4746,  1.0184,  0.9340],\n",
       "          [-0.9685,  0.2611,  0.3602,  ..., -1.4144, -0.7480,  0.7747],\n",
       "          [ 0.1437, -0.4574,  0.4823,  ...,  0.6414, -0.6725,  0.4760]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.3517, -0.1521,  0.0339,  ..., -0.0893, -0.1230, -0.0402],\n",
       "          [-1.2670, -0.3738,  1.2099,  ...,  0.6776, -0.0477,  0.4737],\n",
       "          [-0.6333,  0.0545,  0.7228,  ...,  0.1774,  0.7345,  0.8852],\n",
       "          ...,\n",
       "          [-0.1589,  0.3856, -0.1112,  ...,  0.2170,  0.8159,  0.7138],\n",
       "          [-0.6912, -0.5831,  0.0610,  ..., -1.6318, -0.6512,  1.0126],\n",
       "          [ 0.0154, -0.2261,  0.1721,  ...,  0.1754, -0.2539,  0.0365]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.8029,  0.1422, -0.2297,  ..., -0.3776, -0.0978,  0.1968],\n",
       "          [-1.5378, -0.4223,  1.0376,  ...,  0.6688, -0.3639,  0.7775],\n",
       "          [-0.8042, -0.2304,  0.4107,  ..., -0.0356,  1.1151,  0.8153],\n",
       "          ...,\n",
       "          [-0.2309,  0.3339, -0.2967,  ...,  0.1950,  0.8120,  0.8796],\n",
       "          [-0.2382, -0.6250, -0.0141,  ..., -1.3255, -0.5605,  1.3668],\n",
       "          [ 0.0016, -0.1219,  0.0678,  ..., -0.0286, -0.0854, -0.0075]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.9276,  0.5021, -0.3012,  ..., -0.3072,  0.0585, -0.0952],\n",
       "          [-1.2704, -0.3390,  1.2081,  ...,  0.9522, -0.7325,  0.5915],\n",
       "          [-0.3820, -0.3793,  0.7925,  ...,  0.1308,  0.9165,  0.3711],\n",
       "          ...,\n",
       "          [ 0.0740,  0.0986, -0.3675,  ..., -0.0765,  0.8113,  0.7667],\n",
       "          [ 0.1863, -1.1446,  0.1525,  ..., -1.4336, -0.2183,  0.9785],\n",
       "          [ 0.0383, -0.0120,  0.0225,  ..., -0.0454, -0.0144, -0.0857]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 7.4886e-01,  1.5984e-01, -4.2636e-01,  ...,  3.4759e-01,\n",
       "           -2.3844e-01,  6.5698e-02],\n",
       "          [-1.1580e+00, -8.4134e-01,  1.5521e+00,  ...,  9.8241e-01,\n",
       "           -7.9806e-01,  4.0210e-01],\n",
       "          [-1.3816e-01, -1.6888e-01,  1.1548e+00,  ...,  4.7964e-01,\n",
       "            6.1592e-01,  4.7457e-01],\n",
       "          ...,\n",
       "          [-7.2834e-04,  3.9289e-01,  2.9005e-03,  ...,  5.3336e-02,\n",
       "            8.5969e-01,  8.5849e-01],\n",
       "          [ 4.3715e-01, -9.9634e-01,  1.6856e-01,  ..., -9.4637e-01,\n",
       "           -6.5659e-01,  1.0316e+00],\n",
       "          [-2.1963e-02, -5.5799e-02,  6.2227e-02,  ..., -2.8402e-02,\n",
       "           -4.2231e-02, -1.0502e-01]]], grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.7793,  0.1007, -0.4274,  ...,  0.9048, -0.5227, -0.1137],\n",
       "          [-0.8898, -0.6786,  1.5308,  ...,  1.5091, -0.9785,  0.6784],\n",
       "          [ 0.0086,  0.0768,  1.3342,  ...,  0.9391,  0.4556,  0.6326],\n",
       "          ...,\n",
       "          [-0.2714,  0.9064,  0.0705,  ...,  0.6570,  0.5663,  1.0624],\n",
       "          [-0.1689, -0.8354,  0.0207,  ..., -0.8175, -0.1637,  1.1326],\n",
       "          [-0.0286, -0.0303,  0.0348,  ..., -0.0376, -0.0249, -0.1028]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.5420, -0.0111, -0.4392,  ...,  0.8796, -0.4320,  0.1476],\n",
       "          [-0.8182, -0.0612,  0.9416,  ...,  1.1617, -0.5889,  0.5871],\n",
       "          [-0.0602,  0.7197,  1.2016,  ...,  0.3607,  0.4780,  0.5487],\n",
       "          ...,\n",
       "          [-0.1530,  0.9255,  0.2372,  ...,  0.8295,  0.8583,  1.3533],\n",
       "          [ 0.1905, -0.5354, -0.1243,  ..., -0.5397, -0.3394,  1.1881],\n",
       "          [-0.0707,  0.0226,  0.0754,  ...,  0.0392, -0.0574, -0.0782]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.3221,  0.3998, -0.4169,  ...,  1.0232, -0.5995, -0.0601],\n",
       "          [-1.0271,  0.6184,  1.0531,  ...,  1.9698, -0.6685,  0.6509],\n",
       "          [-0.0514,  1.3279,  1.0187,  ...,  1.0334,  0.2102,  0.3808],\n",
       "          ...,\n",
       "          [-0.1540,  1.3046,  0.5710,  ...,  1.4849,  0.7665,  1.7083],\n",
       "          [-0.1558, -0.1113, -0.1119,  ..., -0.5500, -0.1215,  1.2044],\n",
       "          [-0.0054,  0.0390,  0.0153,  ...,  0.0636, -0.0282, -0.0825]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.8809,  0.7892, -0.4482,  ...,  1.1711,  0.6975, -0.0958],\n",
       "          [ 0.3349,  0.8297,  1.7645,  ...,  1.9044,  0.5226,  1.2158],\n",
       "          [ 0.4887,  1.2486,  1.0644,  ...,  1.3924,  0.7629,  1.0750],\n",
       "          ...,\n",
       "          [ 0.0372,  0.9547,  0.3251,  ...,  1.5483,  0.5109,  1.6606],\n",
       "          [ 0.6295,  0.0969, -0.1112,  ..., -0.1451, -0.3940,  1.2274],\n",
       "          [-0.0465,  0.0894,  0.0296,  ...,  0.0093, -0.0322, -0.0873]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.3667,  0.4766, -1.0618,  ...,  0.8177,  0.6255,  0.0863],\n",
       "          [ 0.6374,  0.4711,  1.5596,  ...,  0.4840,  0.6340,  0.6172],\n",
       "          [ 0.7630,  0.5707,  1.1389,  ...,  0.2344,  0.7438,  0.4282],\n",
       "          ...,\n",
       "          [ 0.1127,  0.3572,  0.4659,  ...,  0.4194,  0.5050,  0.7742],\n",
       "          [ 0.5304, -0.3046, -0.1775,  ..., -0.1200, -0.2799,  0.7286],\n",
       "          [-0.0110,  0.1009,  0.0099,  ..., -0.0421, -0.0176, -0.0560]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[-2.5155e-01,  3.8445e-01, -8.2529e-01,  ...,  3.4392e-01,\n",
       "            7.1555e-01,  7.4832e-01],\n",
       "          [-4.4567e-02,  2.7308e-01,  8.6723e-01,  ...,  4.1996e-01,\n",
       "            6.8162e-01,  8.6585e-01],\n",
       "          [-2.3420e-02,  3.7355e-01,  7.1472e-01,  ...,  4.6169e-02,\n",
       "            9.6745e-01,  6.2572e-01],\n",
       "          ...,\n",
       "          [-6.8192e-01,  2.1714e-01,  3.3690e-01,  ...,  3.1651e-01,\n",
       "            7.7462e-01,  8.4944e-01],\n",
       "          [-2.1414e-01, -2.8565e-01, -1.3242e-01,  ...,  2.2176e-01,\n",
       "            1.9459e-04,  7.2631e-01],\n",
       "          [-1.1367e-01,  1.0859e-01,  4.0207e-02,  ..., -8.7839e-03,\n",
       "           -1.0682e-03, -7.4373e-02]]], grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.1054, -0.3361,  0.7459,  ..., -0.0819,  0.4926,  0.2336],\n",
       "          [ 0.2352, -0.4984,  1.0282,  ..., -0.0698,  0.4869,  0.4297],\n",
       "          [ 0.2079, -0.2714,  0.9021,  ..., -0.1112,  0.5088,  0.3131],\n",
       "          ...,\n",
       "          [-0.0488, -0.4109,  0.6926,  ..., -0.0351,  0.4354,  0.3481],\n",
       "          [-0.0329, -0.5736,  0.6474,  ..., -0.4038,  0.2286,  0.5712],\n",
       "          [-0.3632, -0.2771,  0.9473,  ..., -0.9103,  0.2297, -0.1166]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(ner_df.text.tolist(), is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(ner_df.drop(columns=[\"tags\"]).rename(columns={\"ner_tags\": \"labels\"}))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2679a6af0d4c3ab62ff4573923b243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_to_id = {\"O\":0, \"B-DAT\":1, \"I-DAT\":2}\n",
    "id_to_lbl = {v:k for k,v in lbl_to_id.items()}\n",
    "\n",
    "def convert_ner_tags_to_ids(ner_tags: List[str]) -> List[int]:\n",
    "    return [lbl_to_id[ner_tag] for ner_tag in ner_tags]\n",
    "\n",
    "ds_with_int_labels = ds.map(lambda x: {\"labels\":  [convert_ner_tags_to_ids(lbls) for lbls in x[\"labels\"]]}, batched=True)\n",
    "ds_with_int_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(tokenizer_f, examples):\n",
    "    tokenized_inputs = tokenizer_f(examples[\"text\"])\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = [-100] * len(word_ids) # assume all tokens are special\n",
    "        top_word_id = max(map(lambda x: x if x else -1, word_ids))\n",
    "        for word_idx in range(top_word_id + 1):\n",
    "            label_ids[word_ids.index(word_idx)] = label[word_idx]\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a216e2db36584f3c8bde5afc8c57f067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_f = partial(tokenizer, is_split_into_words=True, truncation=True)\n",
    "ds_tokenized = ds_with_int_labels.map(partial(tokenize_and_align_labels, tokenize_f), batched=True, remove_columns=[\"text\"])\n",
    "ds_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'labels': [-100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100],\n",
       "  'input_ids': [101,\n",
       "   1130,\n",
       "   1901,\n",
       "   1106,\n",
       "   11621,\n",
       "   1704,\n",
       "   6233,\n",
       "   5814,\n",
       "   1869,\n",
       "   117,\n",
       "   2304,\n",
       "   24730,\n",
       "   4454,\n",
       "   1145,\n",
       "   4465,\n",
       "   2233,\n",
       "   1113,\n",
       "   1216,\n",
       "   3050,\n",
       "   1112,\n",
       "   1705,\n",
       "   2060,\n",
       "   117,\n",
       "   3872,\n",
       "   3188,\n",
       "   117,\n",
       "   1105,\n",
       "   1295,\n",
       "   1104,\n",
       "   1278,\n",
       "   6461,\n",
       "   1296,\n",
       "   1768,\n",
       "   1108,\n",
       "   3188,\n",
       "   117,\n",
       "   1111,\n",
       "   1296,\n",
       "   3397,\n",
       "   1214,\n",
       "   1103,\n",
       "   3218,\n",
       "   1108,\n",
       "   4071,\n",
       "   119,\n",
       "   102],\n",
       "  'token_type_ids': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]},\n",
       " {'labels': [-100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   -100,\n",
       "   -100,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100],\n",
       "  'input_ids': [101,\n",
       "   8007,\n",
       "   117,\n",
       "   1160,\n",
       "   118,\n",
       "   11641,\n",
       "   26813,\n",
       "   1320,\n",
       "   1878,\n",
       "   3997,\n",
       "   5715,\n",
       "   4668,\n",
       "   1115,\n",
       "   1412,\n",
       "   3442,\n",
       "   1982,\n",
       "   5409,\n",
       "   1618,\n",
       "   1190,\n",
       "   18911,\n",
       "   11185,\n",
       "   1942,\n",
       "   1111,\n",
       "   6441,\n",
       "   1158,\n",
       "   1103,\n",
       "   23137,\n",
       "   118,\n",
       "   9478,\n",
       "   1174,\n",
       "   3575,\n",
       "   4413,\n",
       "   1107,\n",
       "   2538,\n",
       "   1104,\n",
       "   12120,\n",
       "   2093,\n",
       "   2794,\n",
       "   1113,\n",
       "   1241,\n",
       "   1103,\n",
       "   9960,\n",
       "   12674,\n",
       "   1105,\n",
       "   20452,\n",
       "   3031,\n",
       "   2064,\n",
       "   11781,\n",
       "   2233,\n",
       "   27948,\n",
       "   1114,\n",
       "   4718,\n",
       "   1104,\n",
       "   126,\n",
       "   119,\n",
       "   5391,\n",
       "   193,\n",
       "   1275,\n",
       "   118,\n",
       "   127,\n",
       "   1105,\n",
       "   128,\n",
       "   119,\n",
       "   4573,\n",
       "   193,\n",
       "   1275,\n",
       "   118,\n",
       "   128,\n",
       "   117,\n",
       "   3569,\n",
       "   119,\n",
       "   102],\n",
       "  'token_type_ids': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]},\n",
       " {'labels': [-100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   -100,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100],\n",
       "  'input_ids': [101,\n",
       "   164,\n",
       "   1367,\n",
       "   166,\n",
       "   117,\n",
       "   1300,\n",
       "   112,\n",
       "   17000,\n",
       "   2340,\n",
       "   15328,\n",
       "   112,\n",
       "   7269,\n",
       "   21138,\n",
       "   1104,\n",
       "   14827,\n",
       "   5844,\n",
       "   1718,\n",
       "   1132,\n",
       "   3033,\n",
       "   117,\n",
       "   12619,\n",
       "   5174,\n",
       "   1106,\n",
       "   1129,\n",
       "   1758,\n",
       "   1359,\n",
       "   1113,\n",
       "   1147,\n",
       "   7300,\n",
       "   4267,\n",
       "   8517,\n",
       "   22583,\n",
       "   1116,\n",
       "   1112,\n",
       "   1218,\n",
       "   1112,\n",
       "   1147,\n",
       "   7269,\n",
       "   113,\n",
       "   25128,\n",
       "   8519,\n",
       "   1200,\n",
       "   114,\n",
       "   21138,\n",
       "   119,\n",
       "   102],\n",
       "  'token_type_ids': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]},\n",
       " {'labels': [-100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100],\n",
       "  'input_ids': [101,\n",
       "   16349,\n",
       "   1572,\n",
       "   110,\n",
       "   1104,\n",
       "   1103,\n",
       "   6915,\n",
       "   20744,\n",
       "   1127,\n",
       "   3187,\n",
       "   170,\n",
       "   10840,\n",
       "   117,\n",
       "   3746,\n",
       "   110,\n",
       "   1104,\n",
       "   1103,\n",
       "   6876,\n",
       "   1108,\n",
       "   3187,\n",
       "   8795,\n",
       "   117,\n",
       "   1105,\n",
       "   2588,\n",
       "   110,\n",
       "   1104,\n",
       "   1103,\n",
       "   6876,\n",
       "   1108,\n",
       "   3187,\n",
       "   5788,\n",
       "   113,\n",
       "   1267,\n",
       "   15982,\n",
       "   122,\n",
       "   114,\n",
       "   119,\n",
       "   102],\n",
       "  'token_type_ids': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]},\n",
       " {'labels': [-100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   -100,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   -100,\n",
       "   -100,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   -100,\n",
       "   -100,\n",
       "   -100,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100],\n",
       "  'input_ids': [101,\n",
       "   1109,\n",
       "   7467,\n",
       "   1634,\n",
       "   1108,\n",
       "   10491,\n",
       "   1111,\n",
       "   2967,\n",
       "   5193,\n",
       "   2452,\n",
       "   1106,\n",
       "   17182,\n",
       "   6732,\n",
       "   3484,\n",
       "   1182,\n",
       "   3442,\n",
       "   1106,\n",
       "   121,\n",
       "   119,\n",
       "   4991,\n",
       "   120,\n",
       "   15731,\n",
       "   126,\n",
       "   124,\n",
       "   119,\n",
       "   5073,\n",
       "   2036,\n",
       "   118,\n",
       "   125,\n",
       "   132,\n",
       "   149,\n",
       "   11356,\n",
       "   1108,\n",
       "   12728,\n",
       "   1107,\n",
       "   1103,\n",
       "   3068,\n",
       "   1654,\n",
       "   3343,\n",
       "   119,\n",
       "   102],\n",
       "  'token_type_ids': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]},\n",
       " {'labels': [-100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100],\n",
       "  'input_ids': [101,\n",
       "   1636,\n",
       "   16231,\n",
       "   8296,\n",
       "   1104,\n",
       "   12176,\n",
       "   1193,\n",
       "   2999,\n",
       "   2214,\n",
       "   2833,\n",
       "   117,\n",
       "   1234,\n",
       "   1114,\n",
       "   1346,\n",
       "   1137,\n",
       "   1523,\n",
       "   12029,\n",
       "   2240,\n",
       "   117,\n",
       "   1105,\n",
       "   1234,\n",
       "   1114,\n",
       "   1346,\n",
       "   5844,\n",
       "   119,\n",
       "   102],\n",
       "  'token_type_ids': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]},\n",
       " {'labels': [-100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   -100],\n",
       "  'input_ids': [101,\n",
       "   1109,\n",
       "   19374,\n",
       "   10442,\n",
       "   1110,\n",
       "   1145,\n",
       "   2533,\n",
       "   1118,\n",
       "   2686,\n",
       "   1121,\n",
       "   1103,\n",
       "   142,\n",
       "   2240,\n",
       "   2559,\n",
       "   118,\n",
       "   146,\n",
       "   17095,\n",
       "   2235,\n",
       "   119,\n",
       "   102],\n",
       "  'token_type_ids': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]},\n",
       " {'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100],\n",
       "  'input_ids': [101,\n",
       "   1789,\n",
       "   2877,\n",
       "   1788,\n",
       "   4381,\n",
       "   1136,\n",
       "   3337,\n",
       "   3037,\n",
       "   1112,\n",
       "   1972,\n",
       "   5052,\n",
       "   1132,\n",
       "   1145,\n",
       "   1529,\n",
       "   1107,\n",
       "   1103,\n",
       "   25255,\n",
       "   2103,\n",
       "   119,\n",
       "   102],\n",
       "  'token_type_ids': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]},\n",
       " {'labels': [-100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   0,\n",
       "   -100],\n",
       "  'input_ids': [101,\n",
       "   2096,\n",
       "   1292,\n",
       "   15722,\n",
       "   5174,\n",
       "   113,\n",
       "   5691,\n",
       "   140,\n",
       "   2249,\n",
       "   132,\n",
       "   3882,\n",
       "   12029,\n",
       "   2240,\n",
       "   132,\n",
       "   1955,\n",
       "   5844,\n",
       "   114,\n",
       "   117,\n",
       "   5311,\n",
       "   110,\n",
       "   1104,\n",
       "   140,\n",
       "   2249,\n",
       "   117,\n",
       "   5385,\n",
       "   110,\n",
       "   1104,\n",
       "   12029,\n",
       "   2240,\n",
       "   1105,\n",
       "   5966,\n",
       "   110,\n",
       "   1104,\n",
       "   5844,\n",
       "   2063,\n",
       "   1407,\n",
       "   1808,\n",
       "   1104,\n",
       "   2812,\n",
       "   4455,\n",
       "   117,\n",
       "   1229,\n",
       "   5692,\n",
       "   110,\n",
       "   1104,\n",
       "   140,\n",
       "   2249,\n",
       "   117,\n",
       "   5729,\n",
       "   110,\n",
       "   1104,\n",
       "   12029,\n",
       "   2240,\n",
       "   1105,\n",
       "   3882,\n",
       "   110,\n",
       "   1104,\n",
       "   5844,\n",
       "   2063,\n",
       "   3164,\n",
       "   1808,\n",
       "   1104,\n",
       "   2812,\n",
       "   4455,\n",
       "   119,\n",
       "   102],\n",
       "  'token_type_ids': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]},\n",
       " {'labels': [-100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   -100,\n",
       "   -100,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   -100,\n",
       "   -100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100],\n",
       "  'input_ids': [101,\n",
       "   5395,\n",
       "   1105,\n",
       "   7934,\n",
       "   9833,\n",
       "   1105,\n",
       "   2444,\n",
       "   2531,\n",
       "   1960,\n",
       "   1311,\n",
       "   112,\n",
       "   6724,\n",
       "   1113,\n",
       "   157,\n",
       "   13371,\n",
       "   12480,\n",
       "   131,\n",
       "   1384,\n",
       "   3559,\n",
       "   1105,\n",
       "   4332,\n",
       "   3360,\n",
       "   1107,\n",
       "   170,\n",
       "   1957,\n",
       "   6757,\n",
       "   8519,\n",
       "   1158,\n",
       "   2025,\n",
       "   1529,\n",
       "   1107,\n",
       "   1103,\n",
       "   157,\n",
       "   5123,\n",
       "   3680,\n",
       "   1107,\n",
       "   1570,\n",
       "   9833,\n",
       "   1105,\n",
       "   2444,\n",
       "   8690,\n",
       "   113,\n",
       "   157,\n",
       "   13371,\n",
       "   12480,\n",
       "   114,\n",
       "   1384,\n",
       "   117,\n",
       "   1373,\n",
       "   1114,\n",
       "   1210,\n",
       "   2122,\n",
       "   7112,\n",
       "   117,\n",
       "   1103,\n",
       "   1331,\n",
       "   1104,\n",
       "   11593,\n",
       "   117,\n",
       "   1105,\n",
       "   1141,\n",
       "   1805,\n",
       "   1104,\n",
       "   2722,\n",
       "   119,\n",
       "   102],\n",
       "  'token_type_ids': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ds_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tokenized_collated = data_collator(list(ds_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([[-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "         [-100,    0,    0,    0,    0,    0,    0, -100,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0, -100, -100,    0,    0,\n",
       "          -100,    0,    0,    0,    0, -100,    0,    0,    0,    0,    0,    0,\n",
       "          -100,    0,    0,    0,    0,    0, -100,    0,    0, -100, -100, -100,\n",
       "             0, -100,    0,    0,    0,    0, -100, -100,    0,    0,    0, -100,\n",
       "             0,    0, -100, -100,    0,    0,    0, -100,    0,    0,    0, -100],\n",
       "         [-100,    0,    0,    0,    0,    0,    0,    0, -100, -100,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0, -100, -100, -100,    0,    0,    0,\n",
       "             0,    0,    0,    0, -100, -100,    0,    0,    0, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "         [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "         [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          -100, -100, -100,    0,    0,    0, -100, -100, -100, -100,    0,    0,\n",
       "          -100, -100, -100, -100, -100,    0,    0, -100,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "         [-100,    0,    0,    0,    0,    0, -100,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0, -100,    0,    0,    0,    0,    0,    0,\n",
       "             0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "         [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          -100, -100,    0,    0, -100,    0,    0, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "         [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "         [-100,    0,    0,    0,    0,    0,    0,    0, -100,    0,    0,    0,\n",
       "          -100,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100,    0,\n",
       "             0,    0,    0,    0, -100,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0, -100,    0,    0,    0,    0,    0,    0, -100,    0,\n",
       "             0,    0,    0,    0, -100,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0, -100,    0, -100, -100, -100, -100, -100, -100, -100],\n",
       "         [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0, -100, -100,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0, -100, -100,    0,    0,    0,    0,    1, -100, -100,    2,\n",
       "             2,    2,    2,    2,    2,    0,    0, -100, -100,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0, -100, -100, -100, -100, -100, -100, -100]]),\n",
       " 'input_ids': tensor([[  101,  1130,  1901,  1106, 11621,  1704,  6233,  5814,  1869,   117,\n",
       "           2304, 24730,  4454,  1145,  4465,  2233,  1113,  1216,  3050,  1112,\n",
       "           1705,  2060,   117,  3872,  3188,   117,  1105,  1295,  1104,  1278,\n",
       "           6461,  1296,  1768,  1108,  3188,   117,  1111,  1296,  3397,  1214,\n",
       "           1103,  3218,  1108,  4071,   119,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  8007,   117,  1160,   118, 11641, 26813,  1320,  1878,  3997,\n",
       "           5715,  4668,  1115,  1412,  3442,  1982,  5409,  1618,  1190, 18911,\n",
       "          11185,  1942,  1111,  6441,  1158,  1103, 23137,   118,  9478,  1174,\n",
       "           3575,  4413,  1107,  2538,  1104, 12120,  2093,  2794,  1113,  1241,\n",
       "           1103,  9960, 12674,  1105, 20452,  3031,  2064, 11781,  2233, 27948,\n",
       "           1114,  4718,  1104,   126,   119,  5391,   193,  1275,   118,   127,\n",
       "           1105,   128,   119,  4573,   193,  1275,   118,   128,   117,  3569,\n",
       "            119,   102],\n",
       "         [  101,   164,  1367,   166,   117,  1300,   112, 17000,  2340, 15328,\n",
       "            112,  7269, 21138,  1104, 14827,  5844,  1718,  1132,  3033,   117,\n",
       "          12619,  5174,  1106,  1129,  1758,  1359,  1113,  1147,  7300,  4267,\n",
       "           8517, 22583,  1116,  1112,  1218,  1112,  1147,  7269,   113, 25128,\n",
       "           8519,  1200,   114, 21138,   119,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 16349,  1572,   110,  1104,  1103,  6915, 20744,  1127,  3187,\n",
       "            170, 10840,   117,  3746,   110,  1104,  1103,  6876,  1108,  3187,\n",
       "           8795,   117,  1105,  2588,   110,  1104,  1103,  6876,  1108,  3187,\n",
       "           5788,   113,  1267, 15982,   122,   114,   119,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  1109,  7467,  1634,  1108, 10491,  1111,  2967,  5193,  2452,\n",
       "           1106, 17182,  6732,  3484,  1182,  3442,  1106,   121,   119,  4991,\n",
       "            120, 15731,   126,   124,   119,  5073,  2036,   118,   125,   132,\n",
       "            149, 11356,  1108, 12728,  1107,  1103,  3068,  1654,  3343,   119,\n",
       "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  1636, 16231,  8296,  1104, 12176,  1193,  2999,  2214,  2833,\n",
       "            117,  1234,  1114,  1346,  1137,  1523, 12029,  2240,   117,  1105,\n",
       "           1234,  1114,  1346,  5844,   119,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  1109, 19374, 10442,  1110,  1145,  2533,  1118,  2686,  1121,\n",
       "           1103,   142,  2240,  2559,   118,   146, 17095,  2235,   119,   102,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  1789,  2877,  1788,  4381,  1136,  3337,  3037,  1112,  1972,\n",
       "           5052,  1132,  1145,  1529,  1107,  1103, 25255,  2103,   119,   102,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2096,  1292, 15722,  5174,   113,  5691,   140,  2249,   132,\n",
       "           3882, 12029,  2240,   132,  1955,  5844,   114,   117,  5311,   110,\n",
       "           1104,   140,  2249,   117,  5385,   110,  1104, 12029,  2240,  1105,\n",
       "           5966,   110,  1104,  5844,  2063,  1407,  1808,  1104,  2812,  4455,\n",
       "            117,  1229,  5692,   110,  1104,   140,  2249,   117,  5729,   110,\n",
       "           1104, 12029,  2240,  1105,  3882,   110,  1104,  5844,  2063,  3164,\n",
       "           1808,  1104,  2812,  4455,   119,   102,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  5395,  1105,  7934,  9833,  1105,  2444,  2531,  1960,  1311,\n",
       "            112,  6724,  1113,   157, 13371, 12480,   131,  1384,  3559,  1105,\n",
       "           4332,  3360,  1107,   170,  1957,  6757,  8519,  1158,  2025,  1529,\n",
       "           1107,  1103,   157,  5123,  3680,  1107,  1570,  9833,  1105,  2444,\n",
       "           8690,   113,   157, 13371, 12480,   114,  1384,   117,  1373,  1114,\n",
       "           1210,  2122,  7112,   117,  1103,  1331,  1104, 11593,   117,  1105,\n",
       "           1141,  1805,  1104,  2722,   119,   102,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tokenized_collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82accb3019554078b398f3249f9770b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds_tokenized\u001b[39m.\u001b[39;49mmap(data_collator, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizing-data-ml-algorithms/lib/python3.10/site-packages/datasets/arrow_dataset.py:2585\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2582\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   2584\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2585\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[1;32m   2586\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m   2587\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m   2588\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m   2589\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m   2590\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m   2591\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2592\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m   2593\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m   2594\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   2595\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m   2596\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[1;32m   2597\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   2598\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   2599\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m   2600\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m   2601\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   2602\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[1;32m   2603\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m   2604\u001b[0m     )\n\u001b[1;32m   2605\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2607\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizing-data-ml-algorithms/lib/python3.10/site-packages/datasets/arrow_dataset.py:585\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    584\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    586\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    587\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    588\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizing-data-ml-algorithms/lib/python3.10/site-packages/datasets/arrow_dataset.py:552\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    546\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    547\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    548\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    549\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    550\u001b[0m }\n\u001b[1;32m    551\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 552\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    553\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    554\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizing-data-ml-algorithms/lib/python3.10/site-packages/datasets/fingerprint.py:480\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    478\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    482\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizing-data-ml-algorithms/lib/python3.10/site-packages/datasets/arrow_dataset.py:2982\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2978\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   2979\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(input_dataset\u001b[39m.\u001b[39mnum_rows)))\n\u001b[1;32m   2980\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[1;32m   2981\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2982\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   2983\u001b[0m         batch,\n\u001b[1;32m   2984\u001b[0m         indices,\n\u001b[1;32m   2985\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(input_dataset\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   2986\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m   2987\u001b[0m     )\n\u001b[1;32m   2988\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   2989\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   2990\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2991\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizing-data-ml-algorithms/lib/python3.10/site-packages/datasets/arrow_dataset.py:2865\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2863\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   2864\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 2865\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   2866\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2867\u001b[0m     \u001b[39m# Check if the function returns updated examples\u001b[39;00m\n\u001b[1;32m   2868\u001b[0m     update_data \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable))\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizing-data-ml-algorithms/lib/python3.10/site-packages/datasets/arrow_dataset.py:2545\u001b[0m, in \u001b[0;36mDataset.map.<locals>.decorate.<locals>.decorated\u001b[0;34m(item, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2541\u001b[0m decorated_item \u001b[39m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     Example(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched \u001b[39melse\u001b[39;00m Batch(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)\n\u001b[1;32m   2543\u001b[0m )\n\u001b[1;32m   2544\u001b[0m \u001b[39m# Use the LazyDict internally, while mapping the function\u001b[39;00m\n\u001b[0;32m-> 2545\u001b[0m result \u001b[39m=\u001b[39m f(decorated_item, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2546\u001b[0m \u001b[39m# Return a standard dict\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mdata \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, LazyDict) \u001b[39melse\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizing-data-ml-algorithms/lib/python3.10/site-packages/transformers/data/data_collator.py:45\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf_call(features)\n\u001b[1;32m     44\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch_call(features)\n\u001b[1;32m     46\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnp\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizing-data-ml-algorithms/lib/python3.10/site-packages/transformers/data/data_collator.py:306\u001b[0m, in \u001b[0;36mDataCollatorForTokenClassification.torch_call\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtorch_call\u001b[39m(\u001b[39mself\u001b[39m, features):\n\u001b[1;32m    304\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m     label_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m features[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mkeys() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     labels \u001b[39m=\u001b[39m [feature[label_name] \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m features] \u001b[39mif\u001b[39;00m label_name \u001b[39min\u001b[39;00m features[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    309\u001b[0m         features,\n\u001b[1;32m    310\u001b[0m         padding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizing-data-ml-algorithms/lib/python3.10/site-packages/datasets/arrow_dataset.py:149\u001b[0m, in \u001b[0;36mBatch.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m--> 149\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(key)\n\u001b[1;32m    150\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39mand\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures:\n\u001b[1;32m    151\u001b[0m         values \u001b[39m=\u001b[39m [\n\u001b[1;32m    152\u001b[0m             decode_nested_example(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures[key], value) \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m values\n\u001b[1;32m    153\u001b[0m         ]\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizing-data-ml-algorithms/lib/python3.10/collections/__init__.py:1106\u001b[0m, in \u001b[0;36mUserDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m__missing__\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1105\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__missing__\u001b[39m(\u001b[39mself\u001b[39m, key)\n\u001b[0;32m-> 1106\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "ds_tokenized.map(data_collator, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "data = DataLoader(ds_tokenized, batch_size=1, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([[-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, -100]]), 'input_ids': tensor([[  101,  1130,  1901,  1106, 11621,  1704,  6233,  5814,  1869,   117,\n",
      "          2304, 24730,  4454,  1145,  4465,  2233,  1113,  1216,  3050,  1112,\n",
      "          1705,  2060,   117,  3872,  3188,   117,  1105,  1295,  1104,  1278,\n",
      "          6461,  1296,  1768,  1108,  3188,   117,  1111,  1296,  3397,  1214,\n",
      "          1103,  3218,  1108,  4071,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "for d in data:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 46]), torch.Size([1, 46]), torch.Size([1, 46]))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"labels\"].shape, d[\"input_ids\"].shape, d[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_ds[0][\"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatted_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizingdata-ml-algo/lib/python3.10/site-packages/transformers/data/data_collator.py:42\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf_call(features)\n\u001b[1;32m     41\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch_call(features)\n\u001b[1;32m     43\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnp\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizingdata-ml-algo/lib/python3.10/site-packages/transformers/data/data_collator.py:306\u001b[0m, in \u001b[0;36mDataCollatorForTokenClassification.torch_call\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    304\u001b[0m label_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m features[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m labels \u001b[39m=\u001b[39m [feature[label_name] \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m features] \u001b[39mif\u001b[39;00m label_name \u001b[39min\u001b[39;00m features[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad(\n\u001b[1;32m    307\u001b[0m     features,\n\u001b[1;32m    308\u001b[0m     padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    309\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_length,\n\u001b[1;32m    310\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    311\u001b[0m     \u001b[39m# Conversion to tensors will fail if we have labels as they are not of the same length yet.\u001b[39;49;00m\n\u001b[1;32m    312\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m labels \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    313\u001b[0m )\n\u001b[1;32m    315\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[39mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m~/anaconda3/envs/democratizingdata-ml-algo/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2815\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[39m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[1;32m   2812\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m encoded_inputs:\n\u001b[1;32m   2813\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2814\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 2815\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthat includes \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, but you provided \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(encoded_inputs\u001b[39m.\u001b[39;49mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2816\u001b[0m     )\n\u001b[1;32m   2818\u001b[0m required_input \u001b[39m=\u001b[39m encoded_inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]]\n\u001b[1;32m   2820\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m required_input:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "data_collator(formatted_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[0, 1, 2, 0, 0, 0],\n",
       "         [0, 1, 2, 3, 4, 5]]),\n",
       " 'labels': tensor([[   0,    1,    2, -100, -100, -100],\n",
       "         [   0,    1,    2,    3,    4,    5]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\n",
    "    {\"input_ids\": [0, 1, 2], \"labels\": [0, 1, 2]},\n",
    "    {\"input_ids\": [0, 1, 2, 3, 4, 5], \"labels\": [0, 1, 2, 3, 4, 5]},\n",
    "]\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "democratizing-data-ml-algorithms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b291ab9446582050e02bff38bdb2cc08a6891ecc485df1df216546589a982e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
